# üìä Gu√≠a de Generaci√≥n de Reportes Detallados

Este proyecto incluye herramientas avanzadas para generar reportes detallados de las pruebas automatizadas con Robot Framework.

## üìã Tipos de Reportes Disponibles

### 1. **Reporte HTML Detallado** 
Reporte visual interactivo con estad√≠sticas completas
- **Archivo:** `results/DETAILED_REPORT.html`
- **Contenido:**
  - Resumen visual con tarjetas de m√©tricas
  - Tabla de todas las pruebas
  - An√°lisis de fallos
  - Detalles de keywords ejecutados
  - Timeline de ejecuci√≥n

### 2. **Reportes Est√°ndar de Robot Framework**
- **Reporte HTML:** `results/report.html`
- **Log HTML:** `results/log.html`
- **XML:** `results/output.xml` (datos crudos)

### 3. **Exportaciones en M√∫ltiples Formatos**
Ubicaci√≥n: `results/exports/`

- **JSON:** `results.json` - Datos estructurados completos
- **CSV Tests:** `test_results.csv` - Resultados de pruebas en tabla
- **CSV Keywords:** `keyword_results.csv` - Keywords y tiempos
- **Markdown:** `REPORTE.md` - Reporte legible en texto

### 4. **Dashboard Interactivo**
- **Archivo:** `results/dashboards/index.html`
- **Datos:** `results/dashboards/data/execution_history.json`

## üöÄ Formas de Usar

### Opci√≥n 1: Ejecuci√≥n con Reportes Autom√°ticos (‚≠ê Recomendado)
```bash
python run_and_report.py sandbox
```
O para producci√≥n:
```bash
python run_and_report.py prod
```

**Esto ejecutar√°:**
1. Las pruebas
2. Generar√° el reporte HTML detallado
3. Exportar√° a JSON, CSV y Markdown
4. Actualizar√° el dashboard

### Opci√≥n 2: Generar Reporte Detallado Solamente
Si ya ejecutaste las pruebas y solo necesitas el reporte:
```bash
python generate_detailed_report.py
```

### Opci√≥n 3: Exportar a M√∫ltiples Formatos
```bash
python export_test_reports.py
```

### Opci√≥n 4: Ejecuci√≥n Tradicional + Reportes Manuales
```bash
python run_tests.py sandbox
python generate_detailed_report.py
python export_test_reports.py
```

## üìä Interpretar el Reporte HTML Detallado

### Secci√≥n de Resumen (Summary Cards)
- **Pruebas Totales:** N√∫mero total de casos de prueba
- **Pasadas ‚úì:** Cantidad de pruebas exitosas
- **Fallidas ‚úó:** Cantidad de pruebas con errores
- **Tasa de √âxito:** Porcentaje de pruebas pasadas

### Secci√≥n de Informaci√≥n General
Muestra:
- Tiempo total de ejecuci√≥n
- Hora de inicio y fin
- Pruebas omitidas

### An√°lisis de Fallos
Si hay fallos, muestra:
- **Prueba que fall√≥:** Nombre del test
- **Fall√≥ en:** Keyword donde ocurri√≥ el error
- **Tiempo:** Cu√°nto tiempo llevaba ejecut√°ndose
- **Patrones comunes:** Keywords que fallan frecuentemente

### Detalle de Pruebas
Tabla con:
- Nombre de la prueba
- Estado (PASS/FAIL/SKIP)
- Tiempo de ejecuci√≥n
- Tags asociados

### Keywords Ejecutados
Desglose de cada keyword por prueba:
- Nombre del keyword
- Estado
- Tiempo de ejecuci√≥n

## üìà Exportar Datos para An√°lisis

### JSON - Para Sistemas Externos
```bash
python export_test_reports.py
# Genera: results/exports/results.json
```

Ejemplo de estructura:
```json
{
  "export_date": "2026-01-21T14:30:00",
  "suite": {
    "name": "Automatizacion-Robotframeork-Siman",
    "elapsed_time": 125.45
  },
  "statistics": {
    "total": {
      "passed": 8,
      "failed": 2,
      "total": 10
    }
  },
  "tests": [...]
}
```

### CSV - Para Excel
Los archivos CSV pueden abrirse directamente en Excel:
- `test_results.csv` - An√°lisis por prueba
- `keyword_results.csv` - An√°lisis de performance de keywords

### Markdown - Para Documentaci√≥n
El archivo `REPORTE.md` contiene:
- Resumen ejecutivo
- Listado de pruebas pasadas/fallidas
- Estad√≠sticas por tags
- Keywords m√°s lentos

## üîç Casos de Uso

### Caso 1: Verificar r√°pidamente si las pruebas pasaron
```bash
python run_and_report.py sandbox
# Abre: results/DETAILED_REPORT.html
```

### Caso 2: Analizar un fallo espec√≠fico
1. Abre `results/DETAILED_REPORT.html`
2. Ve a la secci√≥n "An√°lisis de Fallos"
3. Identifica el keyword que fall√≥
4. Abre `results/log.html` para m√°s detalles

### Caso 3: Compartir resultados en reuni√≥n
- Usa `results/exports/REPORTE.md` para presentaci√≥n
- Adjunta `results/DETAILED_REPORT.html` para an√°lisis visual

### Caso 4: Integraci√≥n con CI/CD
```bash
python run_and_report.py sandbox
# Los reportes se generan autom√°ticamente
# Adjunta results/exports/results.json a tu sistema de reporte
```

### Caso 5: An√°lisis hist√≥rico de performance
1. Los datos se guardan en `results/dashboards/data/execution_history.json`
2. El dashboard en `results/dashboards/index.html` muestra tendencias

## ‚öôÔ∏è Personalizar los Reportes

### Modificar Colores
En `generate_detailed_report.py`, busca la secci√≥n `<style>` y modifica los colores:
```css
background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
```

### Agregar Nuevas Secciones
Edita la funci√≥n `_generate_html_report()` en `generate_detailed_report.py`

### Cambiar Formatos de Exportaci√≥n
Modifica `export_test_reports.py` para agregar nuevos formatos

## üìù Notas Importantes

1. Los reportes se generan autom√°ticamente despu√©s de cada ejecuci√≥n
2. El archivo `output.xml` debe existir en `results/` para generar reportes
3. Se recomienda usar `run_and_report.py` para automatizaci√≥n completa
4. Los reportes HTML son independientes y pueden compartirse f√°cilmente
5. Los datos exportados en JSON/CSV pueden integrarse con herramientas externas

## üêõ Soluci√≥n de Problemas

### "No se encontr√≥ output.xml"
- Aseg√∫rate de haber ejecutado primero las pruebas con `python run_tests.py <env>`
- Verifica que exista el archivo `results/output.xml`

### "No se encontr√≥ el archivo de configuraci√≥n"
- Verifica que existan `configs/env_sandbox.yaml` o `configs/env_prod.yaml`

### Los reportes no se actualizan
- Borra la carpeta `results/__pycache__`
- Ejecuta de nuevo: `python run_and_report.py sandbox`

## üìû Soporte

Para reportes adicionales o modificaciones:
1. Edita los archivos Python correspondientes
2. Prueba localmente con `python <script>.py`
3. Ajusta seg√∫n tus necesidades

---

**√öltima actualizaci√≥n:** Enero 2026
